{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "## 1. The Training Process: An Overview\n",
    "\n",
    "Training a neural network follows the same fundamental three-step process as training a logistic regression model.\n",
    "\n",
    "### The 3-Step Process\n",
    "1.  **Specify the Model**: Define the network's architecture (layers, neurons) and how it computes the output $\\hat{y}$ (or $f(\\vec{x})$) from an input $\\vec{x}$. This is the **forward propagation** step.\n",
    "2.  **Specify the Loss and Cost Function**:\n",
    "    * The **Loss Function** $L(f(\\vec{x}), y)$ measures how well the network performs on a *single* training example.\n",
    "    * The **Cost Function** $J(W, B)$ is the average of the loss over the *entire* training set. It measures the performance of the current set of parameters $(W, B)$ on all the data.\n",
    "    $$ J(W,B) = \\frac{1}{m} \\sum_{i=1}^{m} L(f(\\vec{x}^{(i)}), y^{(i)}) $$\n",
    "3.  **Minimize the Cost Function**: Use an optimization algorithm, like **gradient descent**, to find the values of the parameters $W$ and $B$ that minimize the cost $J(W,B)$.\n",
    "\n",
    "### Mapping the Steps to TensorFlow\n",
    "This three-step process maps directly to the primary functions in TensorFlow/Keras:\n",
    "1.  **Specify Model**: `model = tf.keras.Sequential([...])`\n",
    "2.  **Specify Cost**: `model.compile(loss=...)`\n",
    "3.  **Minimize Cost**: `model.fit(X, y, epochs=...)`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Loss and Cost Functions\n",
    "\n",
    "The choice of loss function depends on the type of problem you are solving (classification vs. regression).\n",
    "\n",
    "- **Binary Classification (y=0 or 1)**: The standard loss is **Binary Cross-Entropy**.\n",
    "  $$ L(f(\\vec{x}), y) = -y \\log(f(\\vec{x})) - (1-y) \\log(1 - f(\\vec{x})) $$\n",
    "  In TensorFlow, this is `tf.keras.losses.BinaryCrossentropy`.\n",
    "\n",
    "- **Regression (y is a continuous number)**: The standard loss is **Mean Squared Error**. The loss for a single example is the squared error, and the cost is the average of this over the dataset.\n",
    "  $$ L(f(\\vec{x}), y) = (y - f(\\vec{x}))^2 $$\n",
    "  In TensorFlow, this is `tf.keras.losses.MeanSquaredError`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Minimizing the Cost Function\n",
    "\n",
    "To find the best parameters, we need to minimize $J(W, B)$.\n",
    "- The core algorithm is **gradient descent**, which iteratively updates the parameters to move \"downhill\" on the cost function landscape.\n",
    "- The update rule for any parameter $w_{jk}^{[l]}$ is:\n",
    "  $$ w_{jk}^{[l]} = w_{jk}^{[l]} - \\alpha \\frac{\\partial J(W,B)}{\\partial w_{jk}^{[l]}} $$\n",
    "  (A similar update is performed for the bias parameters $b_j^{[l]}$).\n",
    "- To compute the partial derivatives ($\\frac{\\partial J}{\\partial w}$), neural networks use an efficient algorithm called **backpropagation**. TensorFlow handles this automatically within the `model.fit()` function.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Activation Functions\n",
    "\n",
    "So far we've primarily used the sigmoid function, but there are better choices, especially for hidden layers.\n",
    "\n",
    "### Common Activation Functions\n",
    "1.  **Sigmoid**: $g(z) = \\frac{1}{1 + e^{-z}}$. Output is between 0 and 1. Used for binary classification output layers.\n",
    "2.  **ReLU (Rectified Linear Unit)**: $g(z) = \\max(0, z)$. The most common choice for hidden layers. It's computationally faster and helps mitigate issues with slow learning (vanishing gradients).\n",
    "3.  **Linear**: $g(z) = z$. Essentially no activation function. Used for regression output layers where the output can be positive or negative.\n",
    "\n",
    "### How to Choose Activation Functions: Rules of Thumb\n",
    "- **Output Layer**: The choice depends on the expected output value `y`.\n",
    "    - **Binary Classification** (y is 0 or 1): Use **`sigmoid`**.\n",
    "    - **Regression** (y can be +/-): Use **`linear`**.\n",
    "    - **Regression** (y is non-negative, >= 0): Use **`relu`**.\n",
    "\n",
    "- **Hidden Layers**:\n",
    "    - The default and most common choice is **`relu`**. It generally leads to faster training compared to sigmoid.\n",
    "\n",
    "### Why are Non-Linear Activations Necessary?\n",
    "- A neural network with only **linear activation functions** is mathematically equivalent to a single linear model (like linear or logistic regression).\n",
    "- Stacking linear layers does not increase the model's complexity. The non-linear \"bends\" introduced by functions like ReLU and sigmoid are what allow neural networks to learn highly complex, non-linear relationships in data.\n",
    "- **Rule**: Always use a non-linear activation function (like ReLU) in your hidden layers."
   ],
   "id": "39dfa7f984ca58c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:44:51.887687Z",
     "start_time": "2025-08-02T13:44:49.634846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Model for Binary Classification (e.g., cat vs. dog) ===\n",
    "# This model uses the recommended best practices.\n",
    "model_binary = Sequential([\n",
    "    Dense(units=25, activation='relu'),      # Hidden layer 1: Use ReLU\n",
    "    Dense(units=15, activation='relu'),      # Hidden layer 2: Use ReLU\n",
    "    Dense(units=1, activation='linear')      # Output layer: Use linear (for from_logits)\n",
    "])\n",
    "\n",
    "model_binary.compile(\n",
    "    loss=BinaryCrossentropy(from_logits=True), # Use from_logits=True for numerical stability\n",
    "    optimizer=Adam(learning_rate=0.001)        # Use the Adam optimizer\n",
    ")\n",
    "\n",
    "\n",
    "# === Model for Multiclass Classification (e.g., digits 0-9) ===\n",
    "# This model uses the recommended best practices.\n",
    "model_multiclass = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='linear') # Output layer has 10 units (for 10 classes) and is linear\n",
    "])\n",
    "\n",
    "model_multiclass.compile(\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True), # Use SparseCategoricalCrossentropy with from_logits\n",
    "    optimizer=Adam(learning_rate=0.001)\n",
    ")\n",
    "\n",
    "\n",
    "# === Model for Regression (e.g., house price) ===\n",
    "model_regression = Sequential([\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=50, activation='relu'),\n",
    "    Dense(units=1, activation='linear') # Output is a single number, so linear is a good choice\n",
    "])\n",
    "\n",
    "model_regression.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.01)\n",
    ")\n",
    "\n",
    "# --- Example of making a prediction ---\n",
    "# When using from_logits=True, the model's direct output is 'logits' (raw z values), not probabilities.\n",
    "# To get probabilities, you must manually apply the appropriate activation function.\n",
    "\n",
    "# For binary classification:\n",
    "# logits = model_binary.predict(X_new)\n",
    "# probabilities = tf.nn.sigmoid(logits)\n",
    "\n",
    "# For multiclass classification:\n",
    "# logits = model_multiclass.predict(X_new)\n",
    "# probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "print(\"Models for binary, multiclass, and regression have been defined and compiled.\")"
   ],
   "id": "7533e12a347ed962",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models for binary, multiclass, and regression have been defined and compiled.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Multiclass Classification\n",
    "\n",
    "This applies when you have more than two classes (e.g., classifying digits 0-9).\n",
    "\n",
    "### The Softmax Function\n",
    "Softmax is a generalization of sigmoid to multiple classes. It takes a vector of $N$ real numbers ($z_1, ..., z_N$) and converts it into a probability distribution of $N$ probabilities that sum to 1.\n",
    "- For an output layer with $N$ units (one for each class), the activation for the $j^{th}$ unit is:\n",
    "  $$ a_j = \\frac{e^{z_j}}{\\sum_{k=1}^{N} e^{z_k}} $$\n",
    "- $a_j$ can be interpreted as the probability that the input belongs to class $j$, i.e., $P(y=j | \\vec{x})$.\n",
    "\n",
    "### Loss Function for Softmax\n",
    "The loss function for multiclass classification is **Categorical Cross-Entropy**. If the true class for a training example is $j$, the loss is simply the negative logarithm of the predicted probability for that class:\n",
    "$$ L = -\\log(a_j) $$\n",
    "This encourages the model to assign the highest possible probability to the correct class. In TensorFlow, this is `tf.keras.losses.SparseCategoricalCrossentropy`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Implementation Details\n",
    "\n",
    "### Numerical Stability: `from_logits=True`\n",
    "- When calculating the softmax function and then the cross-entropy loss, intermediate calculations involving exponents ($e^z$) can lead to very large or very small numbers, causing **numerical round-off errors**.\n",
    "- **Solution**: Combine the calculation of the final activation (softmax) and the loss function into a single, more stable step.\n",
    "- **Implementation**:\n",
    "    1. Set the activation function of the final layer to **`linear`** (so it outputs the raw `z` values, called **logits**).\n",
    "    2. In the `compile` step, use the `BinaryCrossentropy` or `SparseCategoricalCrossentropy` loss function and set the argument **`from_logits=True`**.\n",
    "- This is the **recommended practice** for both binary and multiclass classification in TensorFlow for better accuracy.\n",
    "\n",
    "### The Adam Optimizer\n",
    "- **Adam (Adaptive Moment Estimation)** is an optimization algorithm that is generally more effective and faster than standard gradient descent.\n",
    "- It **adapts the learning rate automatically** for each parameter during training.\n",
    "    - It increases the learning rate for parameters that are consistently moving in the same direction.\n",
    "    - It decreases the learning rate for parameters whose updates are oscillating.\n",
    "- Adam is the **de-facto standard optimizer** for training most neural networks today.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Other Layer Types (For Context)\n",
    "- The layers we have used are called **Dense** layers because every neuron is connected to all activations from the previous layer.\n",
    "- Other specialized layers exist, such as **Convolutional Layers**, where neurons only look at a small, localized region of the input. These are the foundation of modern computer vision models. This is for your general knowledge and not required for this course's assignments.\n"
   ],
   "id": "f4e3e6270e288bd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
