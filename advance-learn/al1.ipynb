{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Learning Algorithms: Neural Networks Part 1\n",
    "\n",
    "## 1. Introduction to Neural Networks (NNs)\n",
    "\n",
    "### Motivation and History\n",
    "- **Original Motivation**: To create software that mimics how the human brain learns and thinks. While modern NNs are very different from biological brains, some terminology is inspired by neuroscience.\n",
    "- **Key Brain Component**: The **neuron**, which receives electrical impulses (inputs), performs a computation, and sends outputs to other neurons.\n",
    "- **History**:\n",
    "    - **1950s**: Work began but fell out of favor.\n",
    "    - **1980s-1990s**: Gained popularity again, used in applications like handwritten digit recognition for postal codes.\n",
    "    - **Late 1990s**: Fell out of favor again.\n",
    "    - **~2005 onwards**: Resurgence under the new branding of **Deep Learning**. Since then, NNs have revolutionized fields like speech recognition, computer vision, and natural language processing (NLP).\n",
    "\n",
    "### Why are Neural Networks So Effective Now?\n",
    "The recent success of NNs is driven by two main factors:\n",
    "1.  **Data Scale**: Society has become digitized, generating massive amounts of data (Big Data). Traditional algorithms like linear and logistic regression plateau in performance even with more data. NNs, especially large ones, can continue to improve their performance as the amount of data increases.\n",
    "2.  **Computational Power**: The development of faster CPUs and especially **GPUs (Graphics Processing Units)** has made it feasible to train large neural networks. GPUs were originally for graphics but are highly effective for the matrix computations used in deep learning.\n",
    "\n",
    "![Performance vs Data](https://i.imgur.com/rLoCf2F.png)\n",
    "*(A conceptual graph showing how different sized NNs scale with data compared to traditional ML algorithms)*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Neural Network Model Representation\n",
    "\n",
    "### The Simplest Neuron\n",
    "- A single artificial neuron can be modeled as a **logistic regression unit**.\n",
    "- It takes one or more inputs, performs a computation, and produces a single output.\n",
    "- **Input**: Feature vector $\\vec{x}$.\n",
    "- **Computation**: $z = \\vec{w} \\cdot \\vec{x} + b$\n",
    "- **Output (Activation)**: $a = g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- Here, **'a'** stands for **activation**, a term from neuroscience referring to how strongly a neuron is firing.\n",
    "\n",
    "### Building a Network with Layers\n",
    "A neural network is formed by connecting multiple neurons together in layers.\n",
    "- **Input Layer**: This isn't a computational layer; it simply holds the input features $\\vec{x}$.\n",
    "- **Hidden Layer**: A group of neurons that takes inputs from the previous layer and computes a set of activations. These activations are then passed to the next layer.\n",
    "    - They are called \"hidden\" because their \"correct\" values are not present in the training data; we only see the inputs ($x$) and the final outputs ($y$).\n",
    "- **Output Layer**: The final layer of neurons that produces the network's prediction, $\\hat{y}$.\n",
    "\n",
    "#### Demand Prediction Example\n",
    "Let's predict if a T-shirt will be a top seller based on four features: *price, shipping cost, marketing, and material*.\n",
    "1.  **Input Layer**: $\\vec{x}$ = [price, shipping, marketing, material]\n",
    "2.  **Hidden Layer**: We can design a hidden layer to learn intermediate concepts or features. For example:\n",
    "    - Neuron 1: Learns **affordability** (from price, shipping).\n",
    "    - Neuron 2: Learns **awareness** (from marketing).\n",
    "    - Neuron 3: Learns **perceived quality** (from price, material).\n",
    "3.  **Output Layer**: Takes the outputs (activations) from the hidden layer (affordability, awareness, quality) and makes the final prediction.\n",
    "\n",
    "A key strength of NNs is that we **don't need to manually engineer these hidden features**. The network learns the most useful features by itself during training. In a standard implementation, **every neuron in a layer connects to all outputs from the previous layer**.\n",
    "\n",
    "![NN with one hidden layer](https://i.imgur.com/3D1W7bO.png)\n",
    "\n",
    "### Neural Networks as Feature Learners\n",
    "- A neural network can be viewed as a more powerful version of logistic regression that **learns its own features**.\n",
    "- The hidden layers transform the original input features $\\vec{x}$ into a new, more useful set of features (the activations $\\vec{a}$). The output layer then performs logistic regression on these learned features.\n",
    "- This automates the process of **feature engineering**.\n",
    "\n",
    "### Deeper Networks\n",
    "- A neural network can have multiple hidden layers.\n",
    "- The output of the first hidden layer becomes the input for the second hidden layer, and so on.\n",
    "- A network with multiple hidden layers is often called a **Multilayer Perceptron (MLP)**.\n",
    "- The **architecture** of a network (number of hidden layers and number of neurons per layer) is a key design choice that impacts performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Application: Computer Vision\n",
    "\n",
    "Neural networks excel at learning hierarchical features, which is very powerful for tasks like face recognition.\n",
    "- **Input**: A flattened vector of pixel brightness values from an image. For a 1000x1000 pixel image, this would be a vector with 1,000,000 features.\n",
    "- **Layer 1 (First Hidden Layer)**: Neurons in this layer learn to detect simple patterns like small edges and lines at different orientations.\n",
    "- **Layer 2 (Second Hidden Layer)**: Neurons combine the edges from the previous layer to detect more complex shapes, like eyes, noses, and ears.\n",
    "- **Layer 3 (Third Hidden Layer)**: Neurons combine the facial parts to recognize larger face shapes.\n",
    "- **Output Layer**: Uses the rich set of features from the final hidden layer to identify the person.\n",
    "\n",
    "The remarkable thing is that the network learns this entire hierarchy of features **automatically from the data**. If trained on cars instead of faces, it would learn to detect wheels, windows, etc.\n",
    "\n",
    "![NN for Vision](https://i.imgur.com/K1j11oF.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. A Closer Look at a Layer\n",
    "\n",
    "### Notation\n",
    "To talk about specific layers and neurons, we use the following notation:\n",
    "- $a_j^{[l]}$: Activation of the $j^{th}$ neuron in layer $l$.\n",
    "- $\\vec{w}_j^{[l]}$: Weight vector for the $j^{th}$ neuron in layer $l$.\n",
    "- $b_j^{[l]}$: Bias parameter for the $j^{th}$ neuron in layer $l$.\n",
    "- $\\vec{a}^{[l-1]}$: The vector of activations from the previous layer ($l-1$), which serves as the input to layer $l$.\n",
    "- By convention, the input layer is **Layer 0**, so $\\vec{a}^{[0]} = \\vec{x}$.\n",
    "\n",
    "### Computation for a Single Neuron\n",
    "The computation for a single neuron $j$ in layer $l$ is:\n",
    "$$ z_j^{[l]} = \\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]} $$\n",
    "$$ a_j^{[l]} = g(z_j^{[l]}) $$\n",
    "Where $g$ is the **activation function**. So far, we have used the sigmoid function:\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "### Forward Propagation\n",
    "**Forward propagation** (or inference) is the process of computing the network's output by passing the input data from left to right through the layers.\n",
    "\n",
    "For a network with 2 hidden layers and 1 output layer:\n",
    "1. **Input**: Start with $\\vec{a}^{[0]} = \\vec{x}$.\n",
    "2. **Layer 1**: For each neuron $j$ in layer 1, compute $a_j^{[1]} = g(\\vec{w}_j^{[1]} \\cdot \\vec{x} + b_j^{[1]})$. Collect these into a vector $\\vec{a}^{[1]}$.\n",
    "3. **Layer 2**: For each neuron $j$ in layer 2, compute $a_j^{[2]} = g(\\vec{w}_j^{[2]} \\cdot \\vec{a}^{[1]} + b_j^{[2]})$. Collect these into a vector $\\vec{a}^{[2]}$.\n",
    "4. **Layer 3 (Output)**: Compute $a_1^{[3]} = g(\\vec{w}_1^{[3]} \\cdot \\vec{a}^{[2]} + b_1^{[3]})$.\n",
    "5. **Final Prediction**: The output is $\\hat{y} = a_1^{[3]}$. For binary classification, you can threshold this value at 0.5 to get a prediction of 0 or 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Implementing Neural Networks\n",
    "\n",
    "### Data Representation in NumPy and TensorFlow\n",
    "- **NumPy**: In previous courses, we often used **1D arrays** (e.g., `np.array([200, 17])`) for feature vectors.\n",
    "- **TensorFlow**: Prefers data to be represented as **2D matrices (or tensors)**, even for a single example. This is for computational efficiency.\n",
    "    - A single training example with 2 features would be a `(1, 2)` matrix: `np.array([[200, 17]])`. Notice the double square brackets.\n",
    "    - A `(M, N)` matrix has `M` rows and `N` columns.\n",
    "- **Tensor**: A `tf.Tensor` is TensorFlow's primary data structure, similar to a NumPy array but optimized for running on GPUs/TPUs. You can convert between them using `.numpy()` (Tensor to NumPy) and `tf.convert_to_tensor()` (NumPy to Tensor)."
   ],
   "id": "b0fb891ac3bf9ef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:43:29.784901Z",
     "start_time": "2025-08-02T13:43:27.721914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# === Example: Coffee Roasting (2 features, 2 hidden layers, 1 output) ===\n",
    "\n",
    "# --- Method 1: Layer-by-layer Forward Prop (for demonstration) ---\n",
    "print(\"--- Method 1: Manual Forward Prop ---\")\n",
    "# Assume X is a single data point, correctly shaped as a (1, 2) matrix\n",
    "X_coffee = np.array([[200.0, 17.0]]) # Temperature, Duration\n",
    "\n",
    "# Define Layer 1 (3 units)\n",
    "layer_1 = Dense(units=3, activation='sigmoid')\n",
    "# Define Layer 2 (1 unit)\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Pass data through the layers\n",
    "a1 = layer_1(X_coffee)\n",
    "a2 = layer_2(a1)\n",
    "print(f\"Output a2: {a2.numpy()}\")\n",
    "\n",
    "# Optional: Threshold for binary prediction\n",
    "if a2 >= 0.5:\n",
    "    y_hat = 1\n",
    "else:\n",
    "    y_hat = 0\n",
    "print(f\"Prediction: {y_hat}\")\n",
    "\n",
    "\n",
    "# --- Method 2: Using the Sequential API (the standard way) ---\n",
    "print(\"\\n--- Method 2: Using the Sequential API ---\")\n",
    "# Define the model architecture\n",
    "model_coffee = Sequential([\n",
    "    Dense(units=3, activation='sigmoid', name='L1'),\n",
    "    Dense(units=1, activation='sigmoid', name='L2')\n",
    "])\n",
    "\n",
    "# To use the model, you'd typically train it with .fit()\n",
    "# For now, let's just do prediction (inference)\n",
    "# Note: The weights will be random until the model is trained\n",
    "prediction = model_coffee.predict(X_coffee)\n",
    "print(f\"Prediction from Sequential model: {prediction}\")\n",
    "\n",
    "\n",
    "# === Example: Digit Recognition (64 features, 2 hidden layers, 1 output) ===\n",
    "print(\"\\n--- Digit Recognition Example ---\")\n",
    "# Assume X_digit is a single data point, correctly shaped as a (1, 64) matrix\n",
    "X_digit = np.random.rand(1, 64) # 8x8 image flattened\n",
    "\n",
    "model_digit = Sequential([\n",
    "    Dense(units=25, activation='sigmoid', name='L1'),\n",
    "    Dense(units=15, activation='sigmoid', name='L2'),\n",
    "    Dense(units=1, activation='sigmoid', name='L3_output')\n",
    "])\n",
    "\n",
    "# Make a prediction\n",
    "digit_prediction = model_digit.predict(X_digit)\n",
    "print(f\"Prediction for digit: {digit_prediction}\")"
   ],
   "id": "db86a14fc01eba39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Method 1: Manual Forward Prop ---\n",
      "Output a2: [[0.82609373]]\n",
      "Prediction: 1\n",
      "\n",
      "--- Method 2: Using the Sequential API ---\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step\n",
      "Prediction from Sequential model: [[0.6917728]]\n",
      "\n",
      "--- Digit Recognition Example ---\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "Prediction for digit: [[0.27761093]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Building a Model with TensorFlow\n",
    "TensorFlow is a popular library for deep learning. We use the Keras API within TensorFlow, which is very user-friendly.\n",
    "\n",
    "#### A. Building Layer by Layer (for understanding)\n",
    "You can define each layer and manually pass the data through it.\n",
    "- A standard fully-connected layer is called a **Dense** layer in Keras.\n",
    "- `tf.keras.layers.Dense(units=3, activation='sigmoid')` creates a layer with 3 neurons using the sigmoid activation function.\n",
    "- You then call the layer like a function: `a1 = layer1(x)`.\n",
    "\n",
    "#### B. Building with `tf.keras.Sequential` (the common way)\n",
    "The `Sequential` model is the most common way to build simple, stacked networks. It allows you to define the entire network architecture in one block.\n",
    "- You provide a list of layers to the `Sequential` constructor.\n",
    "- **Training**: `model.compile(...)` and `model.fit(X, y, ...)`.\n",
    "- **Inference**: `model.predict(X_new)`."
   ],
   "id": "738269e9cd778d92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:43:47.950276Z",
     "start_time": "2025-08-02T13:43:47.940629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Implementation of a single dense layer\n",
    "def dense_layer(a_in, W, b):\n",
    "    \"\"\"\n",
    "    Computes the output of a dense layer.\n",
    "    Args:\n",
    "        a_in (ndarray (n,)): Input activations from the previous layer (n features).\n",
    "        W (ndarray (n, m)): Weight matrix, where n is features in, m is units out.\n",
    "        b (ndarray (m,)): Bias vector.\n",
    "    Returns:\n",
    "        a_out (ndarray (m,)): Output activations of the current layer.\n",
    "    \"\"\"\n",
    "    num_units = W.shape[1]\n",
    "    a_out = np.zeros(num_units)\n",
    "    for j in range(num_units):\n",
    "        w_j = W[:, j] # Get the weight vector for the j-th neuron\n",
    "        z_j = np.dot(w_j, a_in) + b[j]\n",
    "        a_out[j] = sigmoid(z_j)\n",
    "    return a_out\n",
    "\n",
    "# Implementation of the full forward propagation\n",
    "def sequential_model(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Performs forward propagation for a 2-layer neural network.\n",
    "    Args:\n",
    "        x (ndarray (n,)): Input features.\n",
    "        W1, b1: Parameters for the first layer.\n",
    "        W2, b2: Parameters for the second layer.\n",
    "    Returns:\n",
    "        a2 (ndarray): Final output/prediction.\n",
    "    \"\"\"\n",
    "    # Compute activations for the first hidden layer\n",
    "    a1 = dense_layer(x, W1, b1)\n",
    "\n",
    "    # Compute activations for the second layer (output layer)\n",
    "    a2 = dense_layer(a1, W2, b2)\n",
    "\n",
    "    return a2\n",
    "\n",
    "# --- Example Usage (Coffee Roasting) ---\n",
    "# Note: In a real scenario, these weights W and biases b would be learned during training.\n",
    "# Here we use placeholder values.\n",
    "\n",
    "# A single input example (1D array for this implementation)\n",
    "x_example = np.array([200.0, 17.0])\n",
    "\n",
    "# Parameters for Layer 1 (2 inputs, 3 units)\n",
    "W1_example = np.array([\n",
    "    [1, -3, 5],\n",
    "    [2, 4, -6]\n",
    "])\n",
    "b1_example = np.array([-1, 1, 2])\n",
    "\n",
    "# Parameters for Layer 2 (3 inputs, 1 unit)\n",
    "W2_example = np.array([\n",
    "    [1],\n",
    "    [2],\n",
    "    [3]\n",
    "])\n",
    "b2_example = np.array([1])\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "prediction = sequential_model(x_example, W1_example, b1_example, W2_example, b2_example)\n",
    "print(f\"\\n--- From-Scratch Implementation ---\")\n",
    "print(f\"Input: {x_example}\")\n",
    "print(f\"Prediction: {prediction}\")"
   ],
   "id": "aa31b14302206e0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- From-Scratch Implementation ---\n",
      "Input: [200.  17.]\n",
      "Prediction: [0.99330715]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implementing Forward Prop from Scratch (Python & NumPy)\n",
    "Understanding the low-level implementation is crucial for debugging and building intuition, even if you primarily use libraries like TensorFlow.\n",
    "- The goal is to create a `dense` function that computes the activations for one layer.\n",
    "- This function will loop through each neuron in the layer, compute its $z$ and $a$ values, and collect them.\n",
    "- A master `sequential` or `predict` function can then call this `dense` function for each layer in the network to perform the full forward pass."
   ],
   "id": "7272a0b0c3b820b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
