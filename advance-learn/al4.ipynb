{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Week 4: Decision Trees\n",
    "\n",
    "## 1. Introduction to Decision Trees\n",
    "\n",
    "A decision tree is a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. It's a flowchart-like structure.\n",
    "\n",
    "### Terminology\n",
    "- **Root Node**: The topmost node, representing the entire dataset, which is the starting point.\n",
    "- **Decision Node**: A node that splits into further nodes. It represents a decision on a particular feature.\n",
    "- **Leaf Node**: A terminal node that does not split further. It represents a class label (in classification) or a continuous value (in regression).\n",
    "\n",
    "![Decision Tree Terminology](https://i.imgur.com/u7qA75N.png)\n",
    "\n",
    "To make a prediction for a new example, you start at the root and traverse down the tree based on the feature values of the example until you reach a leaf node, which gives you the final prediction.\n",
    "\n",
    "## 2. Building a Decision Tree\n",
    "\n",
    "The process of building a decision tree is about recursively splitting the data into purer subsets.\n",
    "\n",
    "### The Core Algorithm\n",
    "1.  Start with all training examples at the root node.\n",
    "2.  **Find the best feature and split**: Calculate the information gain for splitting on every possible feature. Choose the feature that results in the highest information gain.\n",
    "3.  **Split the node**: Create branches for each possible value of the chosen feature.\n",
    "4.  **Repeat**: Recursively apply steps 2 and 3 to each branch until a stopping criterion is met.\n",
    "\n",
    "### How to Choose a Feature to Split?\n",
    "The goal is to choose a split that makes the resulting subsets as \"pure\" as possible (i.e., containing examples of mostly one class).\n",
    "\n",
    "#### Measuring Impurity with Entropy\n",
    "**Entropy** is a measure of impurity or randomness in a set of examples.\n",
    "- For a binary classification problem, where $p_1$ is the fraction of positive examples (class 1), the entropy is:\n",
    "  $$ H(p_1) = -p_1 \\log_2(p_1) - (1-p_1) \\log_2(1-p_1) $$\n",
    "- Entropy is 0 if the set is completely pure (all examples belong to one class, $p_1=0$ or $p_1=1$).\n",
    "- Entropy is 1 (its maximum value) if the set is completely impure (50/50 split, $p_1=0.5$).\n",
    "\n",
    "#### Calculating Information Gain\n",
    "**Information Gain** measures the reduction in entropy achieved by splitting the data on a particular feature. We always choose the split that gives the highest information gain.\n",
    "\n",
    "Let $H_{root}$ be the entropy of the data at the parent node. After splitting, let $w_{left}$ and $w_{right}$ be the fraction of examples going to the left and right branches, and let $H_{left}$ and $H_{right}$ be their respective entropies.\n",
    "\n",
    "$$ \\text{Information Gain} = H_{root} - (w_{left}H_{left} + w_{right}H_{right}) $$\n",
    "\n",
    "### When to Stop Splitting?\n",
    "You stop growing a branch of the tree when one of these conditions is met:\n",
    "- The node is **100% pure** (all examples belong to the same class).\n",
    "- The tree has reached a pre-defined **maximum depth**.\n",
    "- The **information gain** from a split is below a certain threshold.\n",
    "- The number of examples in the node is below a certain threshold.\n",
    "\n",
    "These criteria help prevent the tree from becoming overly complex and overfitting the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Handling Different Feature Types\n",
    "\n",
    "### Categorical Features with >2 Values\n",
    "If a feature can take on more than two discrete values (e.g., ear shape is 'pointy', 'floppy', or 'oval'), we use **One-Hot Encoding**.\n",
    "- A single categorical feature with *k* possible values is replaced by *k* binary features.\n",
    "- Each of the new features corresponds to one of the original possible values. For any given example, exactly one of these new features will be '1' (hot), and the others will be '0'.\n",
    "\n",
    "### Continuous (Numerical) Features\n",
    "To handle continuous features like 'weight', the algorithm finds the best split point by:\n",
    "1.  Sorting all the unique values of the feature in the training data.\n",
    "2.  Considering the midpoint between each adjacent pair of values as a potential split threshold.\n",
    "3.  Calculating the information gain for each potential threshold.\n",
    "4.  Choosing the threshold that provides the highest information gain. The decision node then becomes something like \"weight <= 9.5\".\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Regression Trees (Optional)\n",
    "\n",
    "Decision trees can also be used for regression problems (predicting a continuous value).\n",
    "- **Prediction**: The prediction at a leaf node is the **average** of the target values of all the training examples that fall into that leaf.\n",
    "- **Splitting Criterion**: Instead of reducing entropy, the algorithm seeks to reduce **variance**. The best split is the one that results in the largest **variance reduction**.\n",
    "\n",
    "## 5. Tree Ensembles: The Power of Many\n",
    "\n",
    "A single decision tree can be very sensitive to small changes in the training data. **Tree Ensembles** combine multiple trees to create a more robust and accurate model.\n",
    "\n",
    "### Random Forests\n",
    "The Random Forest algorithm builds multiple decision trees and merges them by averaging their predictions (for regression) or having them vote (for classification). It introduces randomness in two ways:\n",
    "\n",
    "1.  **Sampling with Replacement (Bagging)**: For each tree in the forest, a new training set is created by sampling from the original dataset *with replacement*. This means some examples may appear multiple times, while others may not appear at all.\n",
    "2.  **Random Feature Subsets**: When splitting a node, the algorithm doesn't consider all features. Instead, it chooses a **random subset of features** and finds the best split only from within that subset.\n",
    "\n",
    "These two techniques ensure that the individual trees in the forest are different from each other, which makes the combined prediction more accurate and stable.\n",
    "\n",
    "### Boosted Trees (XGBoost)\n",
    "**Boosting** is an alternative ensemble technique where trees are built sequentially.\n",
    "- The first tree is trained on the data normally.\n",
    "- The second tree is trained to focus more on the examples that the first tree got wrong.\n",
    "- The third tree focuses on the mistakes made by the combination of the first two, and so on.\n",
    "- This process is like \"deliberate practice,\" where the model iteratively corrects its own mistakes.\n",
    "\n",
    "The most popular and powerful implementation of boosted trees is **XGBoost (Extreme Gradient Boosting)**. It is highly efficient, includes regularization to prevent overfitting, and is often a top performer in ML competitions.\n",
    "\n",
    "*After this section, insert the Python code block for XGBoost.*\n",
    "\n",
    "## 6. When to Use Decision Trees vs. Neural Networks?\n",
    "\n",
    "| Decision Trees & Ensembles (XGBoost)                                                              | Neural Networks                                                                                                   |\n",
    "| :-------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------- |\n",
    "| ✅ **Best for structured/tabular data** (like data in spreadsheets).                              | ✅ Works well on **all data types**, including structured data.                                                   |\n",
    "| ❌ Not recommended for **unstructured data** (images, audio, text).                             | ✅ **Best for unstructured data**. This is their key strength.                                                    |\n",
    "| ✅ **Fast to train**, allowing for quicker iteration.                                               | ❌ Can be **slow to train**, especially large networks.                                                           |\n",
    "| ✅ Small, single trees can be **humanly interpretable**. (Ensembles are harder to interpret).         | ❌ Often considered a \"black box\"; harder to interpret.                                                          |\n",
    "| ❌ Does not support transfer learning.                                                              | ✅ **Supports transfer learning**, which is critical when you have a small dataset.                               |\n",
    "\n",
    "**Summary**: Use **XGBoost** for structured/tabular data. Use **Neural Networks** for unstructured data like images, audio, and text, or when you can leverage transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "May the forest be with you."
   ],
   "id": "c62282cf2c6a919c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T14:16:52.469256Z",
     "start_time": "2025-08-02T14:16:49.740370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# You might need to install the library first:\n",
    "# !pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- For Classification ---\n",
    "\n",
    "# 1. Initialize the XGBoost classifier model\n",
    "# Common hyperparameters like n_estimators (number of trees) can be set here.\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "# Assume X_train and y_train are your training data and labels\n",
    "# xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "# predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# --- For Regression ---\n",
    "\n",
    "# 1. Initialize the XGBoost regressor model\n",
    "xgb_regressor = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "# Assume X_train and y_train_reg are your training data and continuous target values\n",
    "# xgb_regressor.fit(X_train, y_train_reg)\n",
    "\n",
    "# 3. Make predictions\n",
    "# numeric_predictions = xgb_regressor.predict(X_test)\n",
    "\n",
    "print(\"XGBoost classifier and regressor models have been defined.\")\n",
    "print(\"Note: The .fit() and .predict() lines are commented out as they require actual data.\")"
   ],
   "id": "88a174ca1a672973",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost classifier and regressor models have been defined.\n",
      "Note: The .fit() and .predict() lines are commented out as they require actual data.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96dccc01b603adba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
