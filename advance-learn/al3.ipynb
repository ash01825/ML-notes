{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Practical Advice for Building Machine Learning Systems\n",
    "\n",
    "## 1. The Core Challenge: Deciding What to Try Next\n",
    "\n",
    "When a trained model's performance is not good enough, there are many possible next steps. The key to being an effective ML practitioner is choosing the right path forward to avoid wasting time.\n",
    "\n",
    "**Common options include:**\n",
    "- Get more training examples.\n",
    "- Try a smaller set of features (to reduce overfitting).\n",
    "- Try a larger set of features (to reduce underfitting).\n",
    "- Add polynomial features ($x_1^2, x_1x_2$, etc.).\n",
    "- Decrease the regularization parameter, $\\lambda$.\n",
    "- Increase the regularization parameter, $\\lambda$.\n",
    "\n",
    "To make good choices, we need **diagnostics**: tests you can run to gain insight into what is or isn't working with your algorithm.\n",
    "\n",
    "## 2. Evaluating a Model\n",
    "\n",
    "The first step is to have a reliable way to measure your model's performance.\n",
    "\n",
    "### Training, Cross-Validation, and Test Sets\n",
    "Instead of using all your data for training, you should split it into three sets:\n",
    "1.  **Training Set (~60%):** Used to train the model's parameters ($W, B$).\n",
    "2.  **Cross-Validation (CV) Set (~20%):** Also called the **validation** or **dev** set. Used to tune hyperparameters (like the degree of a polynomial, $\\lambda$, or neural network architecture) and for diagnostics like bias/variance analysis.\n",
    "3.  **Test Set (~20%):** Used only *once* at the very end to get an unbiased estimate of the final model's real-world performance (generalization error).\n",
    "\n",
    "**Never use the test set to make decisions about the model architecture or hyperparameters.**\n",
    "\n",
    "### Error Metrics\n",
    "We define error functions for each set. For a regression problem, these would be:\n",
    "\n",
    "- **Training Error:**\n",
    "  $$ J_{train}(W,B) = \\frac{1}{2m_{train}} \\sum_{i=1}^{m_{train}} (f_{W,B}(\\vec{x}_{train}^{(i)}) - y_{train}^{(i)})^2 $$\n",
    "\n",
    "- **Cross-Validation Error:**\n",
    "  $$ J_{cv}(W,B) = \\frac{1}{2m_{cv}} \\sum_{i=1}^{m_{cv}} (f_{W,B}(\\vec{x}_{cv}^{(i)}) - y_{cv}^{(i)})^2 $$\n",
    "\n",
    "- **Test Error:**\n",
    "  $$ J_{test}(W,B) = \\frac{1}{2m_{test}} \\sum_{i=1}^{m_{test}} (f_{W,B}(\\vec{x}_{test}^{(i)}) - y_{test}^{(i)})^2 $$\n",
    "\n",
    "**Note:** The cost function $J(W,B)$ used for training includes the regularization term. The error metrics $J_{train}, J_{cv}, J_{test}$ **do not**.\n",
    "\n",
    "For classification, the error is typically the fraction of misclassified examples.\n",
    "\n",
    "### The Model Selection Process\n",
    "1.  Train several different models (e.g., polynomials of different degrees, NNs with different architectures) on the **training set**.\n",
    "2.  Evaluate each of these trained models on the **cross-validation set** using $J_{cv}$.\n",
    "3.  Pick the model that has the lowest $J_{cv}$.\n",
    "4.  Finally, evaluate the chosen model on the **test set** to get a fair estimate of its generalization error.\n",
    "\n",
    "## 3. Diagnosing Bias and Variance\n",
    "\n",
    "This is one of the most powerful diagnostics for understanding model performance.\n",
    "\n",
    "- **High Bias (Underfitting):** The model is too simple and fails to capture the underlying patterns in the data.\n",
    "- **High Variance (Overfitting):** The model is too complex and fits the training data's noise instead of its underlying signal.\n",
    "\n",
    "### Diagnosing with Error Metrics\n",
    "To judge whether your model suffers from high bias or high variance, you first need a **baseline level of performance**. This could be:\n",
    "- Human-level performance.\n",
    "- A competitor's algorithm performance.\n",
    "- An estimate based on prior experience.\n",
    "\n",
    "Let's compare the errors:\n",
    "- **High Bias:** $J_{train}$ is high (significantly worse than the baseline). $J_{cv}$ will also be high, and typically close to $J_{train}$.\n",
    "- **High Variance:** $J_{train}$ is low (at or near baseline performance). $J_{cv}$ is **much higher** than $J_{train}$.\n",
    "- **High Bias AND High Variance:** $J_{train}$ is high, and $J_{cv}$ is even higher. (This is possible with very complex models like NNs).\n",
    "\n",
    "### Learning Curves\n",
    "A learning curve plots the training and cross-validation error as a function of the training set size ($m_{train}$).\n",
    "\n",
    "- **High Bias Learning Curve:** Both $J_{train}$ and $J_{cv}$ are high and they plateau quickly. There is a small gap between them. **Conclusion:** Getting more data will not help.\n",
    "\n",
    "- **High Variance Learning Curve:** There is a large gap between $J_{train}$ (which is low) and $J_{cv}$ (which is high). As the training set size increases, the gap narrows. **Conclusion:** Getting more data is likely to help.\n",
    "\n",
    "![Learning Curves](https://i.imgur.com/GgGvA5K.png)\n",
    "\n",
    "## 4. Fixing Bias and Variance Problems\n",
    "\n",
    "Your diagnosis guides your next steps.\n",
    "\n",
    "| If your model has... | Actions to try...                                                                                                              |\n",
    "| :------------------- | :---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **High Bias** | - **Get additional features**. <br> - **Add polynomial features**. <br> - **Use a more complex model** (e.g., bigger neural network). <br> - **Decrease regularization** ($\\lambda$). |\n",
    "| **High Variance** | - **Get more training data**. <br> - **Try a smaller set of features**. <br> - **Increase regularization** ($\\lambda$).                                          |\n",
    "\n",
    "### A Recipe for Neural Networks\n",
    "Neural networks, when large enough, are \"low bias machines.\" This allows for a simplified workflow:\n",
    "1.  **Does your model have high bias?** (Is $J_{train}$ too high?).\n",
    "    - If yes, make the network bigger (more layers/units). Repeat until bias is low.\n",
    "2.  **Does your model have high variance?** (Is $J_{cv}$ much higher than $J_{train}$?).\n",
    "    - If yes, get more data. Repeat until variance is low.\n",
    "    - Regularization is also key. A larger network with good regularization often performs better than a small one.\n"
   ],
   "id": "f7bc3792ed6d8fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:52:19.478349Z",
     "start_time": "2025-08-02T13:52:17.290433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define a neural network model for handwritten digit classification\n",
    "# with L2 regularization added to the layers.\n",
    "\n",
    "# The regularization strength (lambda) is set inside the l2() function.\n",
    "# A common practice is to start with a small value like 0.01.\n",
    "lambda_val = 0.01\n",
    "\n",
    "model_regularized = Sequential([\n",
    "    Dense(units=25, activation='relu',\n",
    "          kernel_regularizer=l2(lambda_val)), # Add L2 regularization here\n",
    "    Dense(units=15, activation='relu',\n",
    "          kernel_regularizer=l2(lambda_val)), # Add L2 regularization here\n",
    "    Dense(units=10, activation='linear')       # Usually, the output layer is not regularized\n",
    "])\n",
    "\n",
    "# The rest of the compile and fit process remains the same.\n",
    "# For example, for multiclass classification:\n",
    "model_regularized.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "\n",
    "print(\"Model with L2 Regularization has been defined.\")\n",
    "# model_regularized.fit(X_train, y_train, epochs=100) # Example of how you would train it"
   ],
   "id": "eabb8d2bac5f66a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with L2 Regularization has been defined.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. The Full Machine Learning Project Cycle\n",
    "\n",
    "Building a production system is more than just training a model.\n",
    "\n",
    "1.  **Scope Project**: Define what you want to achieve.\n",
    "2.  **Collect Data**: Gather and label your initial dataset.\n",
    "3.  **Train Model**: This is an iterative loop:\n",
    "    - Train your model.\n",
    "    - Perform **Error Analysis** and **Bias/Variance Analysis**.\n",
    "    - Based on diagnostics, improve your model or collect more targeted data.\n",
    "4.  **Deploy & Monitor**: Make the model available to users (e.g., via an API) and monitor its performance on live data. This is part of a field called **MLOps** (Machine Learning Operations).\n",
    "\n",
    "### Error Analysis\n",
    "This is a manual process of examining the examples your algorithm misclassified in the cross-validation set to find common themes. For a spam classifier, you might find that it struggles with:\n",
    "- Pharmaceutical spam (21/100 errors).\n",
    "- Phishing emails (18/100 errors).\n",
    "- Emails with unusual routing (7/100 errors).\n",
    "- Emails with deliberate misspellings (3/100 errors).\n",
    "\n",
    "This analysis tells you that focusing on pharma spam and phishing would be more impactful than spending a lot of time on a sophisticated misspelling detector.\n",
    "\n",
    "### Data-Centric AI\n",
    "While model improvements are important, often the biggest gains come from improving the data.\n",
    "- **Data Augmentation**: Creating new training examples from existing ones. For images, this includes rotating, shearing, changing contrast, etc. For audio, it could mean adding background noise.\n",
    "- **Data Synthesis**: Creating brand new, artificial examples from scratch. For example, using different computer fonts to generate images of text for an OCR system.\n",
    "\n",
    "### Transfer Learning\n",
    "This is a powerful technique for when you don't have much data.\n",
    "1.  **Pre-training**: Take a large, pre-existing neural network that was trained on a massive dataset (e.g., a million images from the internet).\n",
    "2.  **Fine-tuning**:\n",
    "    - Remove the final output layer of the pre-trained network.\n",
    "    - Add a new output layer that matches your specific task (e.g., 10 units for digit classification).\n",
    "    - You can either **freeze** the early layers and only train the new output layer, or you can train all the layers but with a very small learning rate.\n",
    "\n",
    "The intuition is that the pre-trained network has already learned useful low-level features (like edges and shapes for images) that are transferable to your task.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Handling Skewed Classes (Optional but Important)\n",
    "\n",
    "When one class is very rare (e.g., disease diagnosis where <1% of patients are positive), accuracy is a misleading metric.\n",
    "\n",
    "### Precision and Recall\n",
    "We use a **confusion matrix** to define better metrics:\n",
    "\n",
    "|                   | **Actual: 1** | **Actual: 0** |\n",
    "| :---------------- | :----------------- | :----------------- |\n",
    "| **Predicted: 1** | True Positive (TP) | False Positive (FP)|\n",
    "| **Predicted: 0** | False Negative (FN)| True Negative (TN) |\n",
    "\n",
    "- **Precision**: Of all the examples we predicted as positive, what fraction were actually positive?\n",
    "  $$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "\n",
    "- **Recall**: Of all the actual positive examples, what fraction did we correctly identify?\n",
    "  $$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "\n",
    "### Precision-Recall Tradeoff\n",
    "You can change the prediction threshold (default is 0.5) to trade off between precision and recall:\n",
    "- **High Threshold (e.g., 0.9):** Predict 1 only when very confident -> High Precision, Low Recall.\n",
    "- **Low Threshold (e.g., 0.3):** Predict 1 even when not very confident -> Low Precision, High Recall.\n",
    "\n",
    "### F1 Score\n",
    "To combine precision (P) and recall (R) into a single number, we use the **F1 Score**, which is the harmonic mean of the two. It penalizes models that have very low P or R.\n",
    "$$ F_1 \\text{ Score} = 2 \\frac{P \\cdot R}{P + R} $$\n",
    "A higher F1 score is generally better. This allows you to compare models or different thresholds automatically."
   ],
   "id": "e2677c957ae1b346"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
