{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear Regression with Multiple Features & Practical Considerations (Part 2)\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "* **Purpose:** Predict $y$ (output) using **multiple features** ($x_1, x_2, \\dots, x_n$) instead of just one.\n",
    "* **Notation:**\n",
    "    * $x_j$: The $j$-th feature.\n",
    "    * $n$: Total number of features.\n",
    "    * $x^{(i)}$: The $i$-th training example (a list/vector of features).\n",
    "    * $x_j^{(i)}$: The value of the $j$-th feature for the $i$-th training example.\n",
    "* **Model Equation:**\n",
    "    $$ f_{w,b}(X) = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b $$\n",
    "    * **Interpretation of Parameters:**\n",
    "        * $b$: Base price (y-intercept when all features are zero).\n",
    "        * $w_j$: Impact of the $j$-th feature on the predicted output (e.g., $w_1=0.1$ means $0.1 \\times \\$1000 = \\$100$ increase per square foot).\n",
    "* **Vectorized Notation:**\n",
    "    * Collect parameters $w_1, \\dots, w_n$ into a **vector** $W$.\n",
    "    * Collect features $x_1, \\dots, x_n$ into a **vector** $X$.\n",
    "    * Model can be written as:\n",
    "        $$ f_{W,b}(X) = W \\cdot X + b $$\n",
    "        * **Dot Product:** $W \\cdot X = w_1x_1 + w_2x_2 + \\dots + w_nx_n$.\n",
    "* **Name:** This model is called **Multiple Linear Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "* **Purpose:** Makes code shorter, easier to read, and significantly faster.\n",
    "* **Mechanism:** Leverages optimized numerical linear algebra libraries (like **NumPy**) and parallel processing hardware (CPU, GPU) by operating on entire arrays/vectors/matrices at once.\n",
    "* **NumPy Indexing:** Starts from `0` (e.g., `w[0]`, `x[0]`).\n",
    "* **Implementation Example:**\n",
    "    * **Non-vectorized (for-loop):**\n",
    "        ```python\n",
    "        f = 0\n",
    "        for j in range(n):\n",
    "            f = f + w[j] * x[j]\n",
    "        f = f + b\n",
    "        ```\n",
    "    * **Vectorized (NumPy dot product):**\n",
    "        ```python\n",
    "        f = np.dot(w, x) + b\n",
    "        ```\n",
    "\n",
    "---"
   ],
   "id": "cadd298d549cd091"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T11:58:13.931983Z",
     "start_time": "2025-08-02T11:58:13.860801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Demo 1: Vectorization Speed Comparison ---\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"--- Demo 1: Vectorization Speed Comparison ---\")\n",
    "\n",
    "# Example parameters and features for a small case (n=3)\n",
    "w_small = np.array([0.1, 4.0, 10.0])\n",
    "b_small = 80\n",
    "x_small = np.array([1000, 3, 2])\n",
    "\n",
    "# Non-vectorized prediction for small case\n",
    "f_non_vec_small = 0\n",
    "for j in range(len(w_small)):\n",
    "    f_non_vec_small += w_small[j] * x_small[j]\n",
    "f_non_vec_small += b_small\n",
    "print(f\"Non-vectorized (small n) prediction: {f_non_vec_small:.2f}\")\n",
    "\n",
    "# Vectorized prediction for small case\n",
    "f_vec_small = np.dot(w_small, x_small) + b_small\n",
    "print(f\"Vectorized (small n) prediction: {f_vec_small:.2f}\\n\")\n",
    "\n",
    "\n",
    "# Simulate a large number of features for speed test (e.g., n = 100,000)\n",
    "n_features_large = 100000\n",
    "w_large = np.random.rand(n_features_large) # Random weights\n",
    "x_large = np.random.rand(n_features_large) # Random features\n",
    "b_large = 50\n",
    "\n",
    "# Time non-vectorized computation\n",
    "start_time_non_vec = time.time()\n",
    "f_large_non_vec = 0\n",
    "for j in range(n_features_large):\n",
    "    f_large_non_vec += w_large[j] * x_large[j]\n",
    "f_large_non_vec += b_large\n",
    "end_time_non_vec = time.time()\n",
    "time_non_vec = end_time_non_vec - start_time_non_vec\n",
    "print(f\"Time for non-vectorized (n={n_features_large}): {time_non_vec:.6f} seconds\")\n",
    "\n",
    "# Time vectorized computation\n",
    "start_time_vec = time.time()\n",
    "f_large_vec = np.dot(w_large, x_large) + b_large\n",
    "end_time_vec = time.time()\n",
    "time_vec = end_time_vec - start_time_vec\n",
    "print(f\"Time for vectorized (n={n_features_large}): {time_vec:.6f} seconds\")\n",
    "\n",
    "# Compare speeds\n",
    "if time_vec > 0: # Avoid division by zero if time_vec is extremely small\n",
    "    print(f\"\\nVectorized code is approximately {time_non_vec / time_vec:.1f}x faster!\")\n",
    "else:\n",
    "    print(\"\\nVectorized code was extremely fast, couldn't accurately measure speedup.\")"
   ],
   "id": "59a423d072b89568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Demo 1: Vectorization Speed Comparison ---\n",
      "Non-vectorized (small n) prediction: 212.00\n",
      "Vectorized (small n) prediction: 212.00\n",
      "\n",
      "Time for non-vectorized (n=100000): 0.017076 seconds\n",
      "Time for vectorized (n=100000): 0.000029 seconds\n",
      "\n",
      "Vectorized code is approximately 587.1x faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "* **Objective:** Minimize the cost function $J(W, b)$ to find optimal parameters $W$ and $b$.\n",
    "* **Update Rule for each $w_j$ ($j=1, \\dots, n$):**\n",
    "    $$ w_j := w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(W, b) $$\n",
    "    * Where the partial derivative for $w_j$ is:\n",
    "        $$ \\frac{\\partial}{\\partial w_j} J(W, b) = \\frac{1}{m} \\sum_{i=1}^{m} (f_{W,b}(x^{(i)}) - y^{(i)})x_j^{(i)} $$\n",
    "* **Update Rule for $b$:**\n",
    "    $$ b := b - \\alpha \\frac{\\partial}{\\partial b} J(W, b) $$\n",
    "    * Where the partial derivative for $b$ is:\n",
    "        $$ \\frac{\\partial}{\\partial b} J(W, b) = \\frac{1}{m} \\sum_{i=1}^{m} (f_{W,b}(x^{(i)}) - y^{(i)}) $$\n",
    "* **Key Point:** All parameters ($w_1, \\dots, w_n, b$) must be **updated simultaneously** in each step.\n",
    "\n",
    "### Normal Equation (Alternative for Linear Regression)\n",
    "\n",
    "* **Method:** A non-iterative approach using linear algebra to directly solve for $W$ and $b$.\n",
    "* **Applies To:** **Only linear regression models**.\n",
    "* **Disadvantages:**\n",
    "    * Does **not generalize** to other learning algorithms (e.g., logistic regression, neural networks).\n",
    "    * Can be **slow** if the number of features ($n$) is very large.\n",
    "* **Use Case:** Some mature machine learning libraries might use this internally for linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "* **Problem:** If features have vastly different ranges (e.g., house size: 300-2000 sq ft; bedrooms: 0-5), the cost function's contour plot will be **elongated/skinny**. This makes Gradient Descent **slow** and prone to \"bouncing\" back and forth.\n",
    "* **Solution:** Transform features so they have **comparable ranges of values**.\n",
    "* **Benefit:** Greatly **speeds up** Gradient Descent's convergence.\n",
    "* **Rule of Thumb:** Aim for feature ranges roughly between **-1 and +1** (or similar small, symmetric ranges like -3 to +3, -0.3 to +0.3). Features with very large (e.g., -100 to +100) or very small (e.g., -0.001 to +0.001) ranges should likely be scaled. When in doubt, scale.\n",
    "\n",
    "### Common Feature Scaling Methods\n",
    "\n",
    "1.  **Scaling by Max:**\n",
    "    $$ x_j^{\\text{scaled}} = \\frac{x_j}{\\text{max}(x_j)} $$\n",
    "2.  **Mean Normalization:** Centers features around zero.\n",
    "    $$ x_j^{\\text{normalized}} = \\frac{x_j - \\mu_j}{\\text{max}(x_j) - \\text{min}(x_j)} $$\n",
    "    * $\\mu_j$: Mean (average) of feature $j$.\n",
    "3.  **Z-score Normalization:** Centers features around zero and scales by standard deviation.\n",
    "    $$ x_j^{\\text{normalized}} = \\frac{x_j - \\mu_j}{\\sigma_j} $$\n",
    "    * $\\sigma_j$: Standard deviation of feature $j$.\n",
    "\n",
    "---"
   ],
   "id": "12c028a3ca3517ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T11:58:27.979346Z",
     "start_time": "2025-08-02T11:58:27.969440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Demo 2: Feature Scaling Examples ---\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Demo 2: Feature Scaling Examples ---\")\n",
    "\n",
    "# Example data for house size (x1) and number of bedrooms (x2)\n",
    "# Units: x1 in sq ft (e.g., 300 to 2000), x2 is count (e.g., 0 to 5)\n",
    "x1_original = np.array([300, 800, 1200, 1500, 2000])\n",
    "x2_original = np.array([0, 1, 2, 3, 5])\n",
    "\n",
    "print(f\"Original x1: {x1_original}\")\n",
    "print(f\"Original x2: {x2_original}\\n\")\n",
    "\n",
    "# 1. Scaling by Max\n",
    "x1_scaled_max = x1_original / np.max(x1_original)\n",
    "x2_scaled_max = x2_original / np.max(x2_original)\n",
    "print(f\"Scaled by Max x1: {x1_scaled_max.round(3)}\")\n",
    "print(f\"Scaled by Max x2: {x2_scaled_max.round(3)}\\n\")\n",
    "\n",
    "# 2. Mean Normalization\n",
    "mu1 = np.mean(x1_original)\n",
    "range1 = np.max(x1_original) - np.min(x1_original)\n",
    "x1_mean_norm = (x1_original - mu1) / range1\n",
    "\n",
    "mu2 = np.mean(x2_original)\n",
    "range2 = np.max(x2_original) - np.min(x2_original)\n",
    "x2_mean_norm = (x2_original - mu2) / range2\n",
    "\n",
    "print(f\"Mean Normalized x1 (mean={mu1:.1f}, range={range1:.1f}): {x1_mean_norm.round(3)}\")\n",
    "print(f\"Mean Normalized x2 (mean={mu2:.1f}, range={range2:.1f}): {x2_mean_norm.round(3)}\\n\")\n",
    "\n",
    "# 3. Z-score Normalization\n",
    "std1 = np.std(x1_original)\n",
    "x1_zscore = (x1_original - mu1) / std1\n",
    "\n",
    "std2 = np.std(x2_original)\n",
    "x2_zscore = (x2_original - mu2) / std2\n",
    "\n",
    "print(f\"Z-score Normalized x1 (mean={mu1:.1f}, std={std1:.1f}): {x1_zscore.round(3)}\")\n",
    "print(f\"Z-score Normalized x2 (mean={mu2:.1f}, std={std2:.1f}): {x2_zscore.round(3)}\\n\")"
   ],
   "id": "65b26e73f73177c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Demo 2: Feature Scaling Examples ---\n",
      "Original x1: [ 300  800 1200 1500 2000]\n",
      "Original x2: [0 1 2 3 5]\n",
      "\n",
      "Scaled by Max x1: [0.15 0.4  0.6  0.75 1.  ]\n",
      "Scaled by Max x2: [0.  0.2 0.4 0.6 1. ]\n",
      "\n",
      "Mean Normalized x1 (mean=1160.0, range=1700.0): [-0.506 -0.212  0.024  0.2    0.494]\n",
      "Mean Normalized x2 (mean=2.2, range=5.0): [-0.44 -0.24 -0.04  0.16  0.56]\n",
      "\n",
      "Z-score Normalized x1 (mean=1160.0, std=581.7): [-1.478 -0.619  0.069  0.584  1.444]\n",
      "Z-score Normalized x2 (mean=2.2, std=1.7): [-1.279 -0.697 -0.116  0.465  1.627]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Debugging Gradient Descent: Convergence\n",
    "\n",
    "* **Objective:** Verify Gradient Descent is correctly minimizing the cost function.\n",
    "* **Method:** Plot the **cost function $J$ vs. the number of iterations**. This plot is called a **learning curve**.\n",
    "    * Horizontal axis: Number of iterations.\n",
    "    * Vertical axis: Cost $J$.\n",
    "\n",
    "* **Expected Behavior (Proper Convergence):**\n",
    "    * Cost $J$ must **decrease on every single iteration**.\n",
    "    * The curve should eventually **flatten out**, indicating convergence to a minimum.\n",
    "    * Number of iterations to converge varies by application (tens to hundreds of thousands).\n",
    "\n",
    "* **Signs of Problems:**\n",
    "    * **Cost $J$ increases** on any iteration, or **fluctuates (goes up and down):**\n",
    "        * Likely $\\alpha$ (learning rate) is **too large**.\n",
    "        * Could indicate a **bug in the code** (e.g., using `+` instead of `-` in the update rule).\n",
    "    * **Cost $J$ decreases very slowly:**\n",
    "        * $\\alpha$ is **too small**.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Learning Rate ($\\alpha$)\n",
    "\n",
    "* **Importance:** Crucial for efficient and correct convergence.\n",
    "* **Debugging Tip:** If gradient descent isn't working, set $\\alpha$ to a **very small number** (e.g., 0.0001). If $J$ still increases, there's likely a bug in your code.\n",
    "* **Finding Optimal $\\alpha$:**\n",
    "    1.  **Try a range of values:** Start small (e.g., 0.001).\n",
    "    2.  **Increment by factors of 3 (or 10):** Example sequence: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3...\n",
    "    3.  **Plot learning curves** for each $\\alpha$ (run for a handful of iterations).\n",
    "    4.  **Choose:** The largest $\\alpha$ that results in a **rapid but consistently decreasing** cost function.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering and Polynomial Regression\n",
    "\n",
    "* **Feature Engineering:** Designing new features by transforming or combining existing ones, using your knowledge about the problem.\n",
    "    * **Example:** Create `area = lot_width * lot_depth`. Use `area` as a new feature.\n",
    "    * **Benefit:** Can significantly improve model accuracy by providing more relevant information.\n",
    "\n",
    "* **Polynomial Regression:** A type of feature engineering for linear regression to fit **non-linear curves**.\n",
    "    * **Idea:** Create new features by raising existing features to different powers (e.g., $x, x^2, x^3$).\n",
    "    * **Example:** Model house price based on `size` ($x$), `size^2` ($x^2$), `size^3` ($x^3$).\n",
    "        * Model: $f_{W,b}(X) = w_1x + w_2x^2 + w_3x^3 + b$.\n",
    "        * This is still linear in its parameters but fits a non-linear curve with respect to the original feature $x$.\n",
    "    * **Critical Note:** When using polynomial features, **feature scaling is even more important** due to vastly different ranges (e.g., $x$ from 1-1000, $x^2$ from 1-1,000,000).\n",
    "    * **Other options:** Use functions like $\\sqrt{x}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Scikit-learn (Mentioned)\n",
    "\n",
    "* A widely used open-source Python ML library.\n",
    "* Provides ready-to-use implementations for algorithms like linear regression.\n",
    "* Understanding manual implementation is still important for solid comprehension and debugging, even when using libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "* **Underfitting (High Bias):**\n",
    "    * **Definition:** Model is too simple; it **doesn't fit the training data well** and misses clear patterns.\n",
    "    * **Characteristics:** Poor performance on both training and new data.\n",
    "    * **Example:** Fitting a straight line to data that clearly needs a curve.\n",
    "* **Overfitting (High Variance):**\n",
    "    * **Definition:** Model is too complex; it **fits the training data \"too well\"** (even noise), but performs poorly on new, unseen data.\n",
    "    * **Characteristics:** Excellent performance on training data, but poor **generalization**.\n",
    "    * **Example:** Fitting a very high-order polynomial that creates a \"wiggly\" curve through all training points.\n",
    "* **\"Just Right\" Model:** Balances fitting training data well (low bias) and generalizing well (low variance). This is the goal.\n",
    "\n",
    "---\n",
    "\n",
    "## Addressing Overfitting\n",
    "\n",
    "1.  **Collect More Training Data:**\n",
    "    * **Effect:** Helps complex models learn more general patterns.\n",
    "    * **Feasibility:** Not always possible.\n",
    "\n",
    "2.  **Use Fewer Features (Feature Selection):**\n",
    "    * **Method:** Manually select a subset of the most relevant features.\n",
    "    * **Disadvantage:** May discard useful information.\n",
    "\n",
    "3.  **Regularization:**\n",
    "    * **Core Idea:** Encourage the learning algorithm to keep **parameter values ($w_j$) small** (but not necessarily zero).\n",
    "    * **Effect:** A model with smaller parameters tends to be \"simpler\" and \"smoother,\" reducing overfitting.\n",
    "    * **Convention:** Typically regularize $w_1, \\dots, w_n$, but not the bias parameter $b$.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization Cost Function\n",
    "\n",
    "* **Modified Cost Function:** Adds a **regularization term** to the original cost function.\n",
    "* **For Linear Regression (Regularized Mean Squared Error):**\n",
    "    $$ J(W, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{W,b}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n",
    "* **For Logistic Regression (Regularized Log Loss):**\n",
    "    $$ J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log(f_{W,b}(x^{(i)})) + (1-y^{(i)})\\log(1-f_{W,b}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n",
    "* **Components:**\n",
    "    1.  **Original Cost Term:** Measures how well the model fits the training data.\n",
    "    2.  **Regularization Term:** Penalizes large parameter values ($w_j^2$).\n",
    "* **$\\lambda$ (Lambda): Regularization Parameter:**\n",
    "    * Controls the **trade-off** between fitting the training data and keeping parameters small.\n",
    "    * **$\\lambda = 0$:** No regularization, prone to overfitting.\n",
    "    * **$\\lambda$ is very large:** Strong regularization, forces $w_j$ close to 0, prone to underfitting.\n",
    "    * **\"Just right\" $\\lambda$:** Balances the two goals for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Descent with Regularization\n",
    "\n",
    "* **Update Rule for each $w_j$ ($j=1, \\dots, n$):**\n",
    "    $$ w_j := w_j - \\alpha \\left[ \\left( \\frac{1}{m} \\sum_{i=1}^{m} (f_{W,b}(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j \\right] $$\n",
    "    * **Intuition:** Each $w_j$ is slightly shrunk by multiplying it with $(1 - \\alpha \\frac{\\lambda}{m})$ before applying the usual update.\n",
    "* **Update Rule for $b$ (No Regularization):**\n",
    "    $$ b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{W,b}(x^{(i)}) - y^{(i)}) $$\n",
    "* **Important:**\n",
    "    * The definition of $f_{W,b}(x)$ depends on the model (linear vs. logistic regression).\n",
    "    * Always perform **simultaneous updates** for all parameters.\n",
    "    * **Feature scaling** remains crucial for faster convergence.\n",
    "\n",
    "---"
   ],
   "id": "3c31365b7df147c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
